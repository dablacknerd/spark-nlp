{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic binary text classification with logistic regression and spark-nlp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BLniM7FtR_2"
      },
      "source": [
        "**Mounting google drive so i can access the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCbBU0pLsWfm",
        "outputId": "f84479ff-e52b-44bb-b36d-10cb62e6e802",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/gdrive')\n",
        "data_dir = os.path.join(os.getcwd(),'gdrive','My Drive','Colab Notebooks','Text_Data')\n",
        "file_path = os.path.join(data_dir,'sarcasm_headline_dataset.csv')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HzGtIimtcl8"
      },
      "source": [
        "**Setting up pyspark and spark-nlp on google-colabs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGXvWfF5s9b9",
        "outputId": "1782a62a-e6e4-41f4-ae9d-a883994b3537",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Install java\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "# Install pyspark\n",
        "! pip install --ignore-installed -q pyspark==2.4.4\n",
        "# Install Spark NLP\n",
        "! pip install --ignore-installed -q spark-nlp==2.4.5 "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_272\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_272-8u272-b10-0ubuntu1~18.04-b10)\n",
            "OpenJDK 64-Bit Server VM (build 25.272-b10, mixed mode)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du8mHdwztwks"
      },
      "source": [
        "**Importing spark-nlp libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VExtK-IGtG8I"
      },
      "source": [
        "import sparknlp\n",
        "from sparknlp.base import DocumentAssembler,Finisher\n",
        "from sparknlp.annotator import SentenceDetector, Tokenizer, Normalizer, StopWordsCleaner, Lemmatizer"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOqUjZxiuU8i"
      },
      "source": [
        "**Importing pyspark libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8EDPFpluUbN"
      },
      "source": [
        "from pyspark.sql.functions import * \n",
        "from pyspark.sql.types import * \n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import *\n",
        "from pyspark.ml.feature import Word2Vec,StringIndexer\n",
        "from pyspark.ml import Pipeline"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at6b32gbuoOE"
      },
      "source": [
        "**Loading data and doing some sanity checking**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mDeoDJAuiW5",
        "outputId": "5a6f3a7d-58e1-4bf7-b505-0973ae110b16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "spark = sparknlp.start() \n",
        "\n",
        "df = spark.read.csv(file_path,inferSchema=True,header=True)\n",
        "df.printSchema()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- HEADLINE: string (nullable = true)\n",
            " |-- IS_SARCASTIC: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSich_vCuvRn",
        "outputId": "3f2d3837-53db-4ff1-a6e3-04c0feb563d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Number of rows(sentences): {}\".format(df.count()))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows(sentences): 26709\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNkgQhzBvPnK",
        "outputId": "3a8c4c33-6956-43ea-e201-8a47d75ac4f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.toPandas()['IS_SARCASTIC'].value_counts()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                                      14976\n",
              "1                                                      11723\n",
              " honey?\"\"\"                                                 1\n",
              " report finds\"                                             1\n",
              " hate the sin\"\" won't cut it anymore\"                      1\n",
              "\"\" says stern woman who likely does not menstruate\"        1\n",
              " i'm sorry                                                 1\n",
              "\"\" sugar \"\"daddy                                           1\n",
              " they're a part of it\"\"\"                                   1\n",
              " at $40                                                    1\n",
              "\"\" but nobody's laughing\"                                  1\n",
              "\"\" \"\"the mapmaker's opera                                  1\n",
              "Name: IS_SARCASTIC, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZu5efaYvzxw"
      },
      "source": [
        "**The \"IS_SARCASTIC\" column should contain either a 0 (not a sarcastic headline) or 1 (a sarcastic headline). Use filter to drop rows(10) that aren't a 0 or 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkqYcE5Tvf0o",
        "outputId": "6baf6f58-a7fd-4f8a-f6ec-de1bcd0dbc29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df = df.filter((df[\"IS_SARCASTIC\"] == '0') | (df[\"IS_SARCASTIC\"] == '1'))\n",
        "df.toPandas()['IS_SARCASTIC'].value_counts()\n",
        "print(\"Number of rows(sentences) after filtering: {}\".format(df.count()))\n",
        "\n",
        "# Splitting the data into a train and test split.\n",
        "# My main focus here is on preprocessing not model performance.\n",
        "# If model performance were my main focus, i would do a train, test, validation split\n",
        "train,test = df.randomSplit([0.7,0.3],seed=42)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows(sentences) after filtering: 26699\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2ujcpscw41H"
      },
      "source": [
        "**Building the spark-nlp pipeline that will preprocess the sentences into the format they need to be in before deriving word2vec embeddings from them**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE1Ex9YLwfgQ"
      },
      "source": [
        "docAssembler = DocumentAssembler().setInputCol(\"HEADLINE\") \\\n",
        "                                  .setOutputCol(\"document\") \\\n",
        "                                  .setCleanupMode(\"shrink_full\")\n",
        "\n",
        "sentence_detector = SentenceDetector().setInputCols([\"document\"]).setOutputCol(\"sentence\")\n",
        "\n",
        "tokenizer = Tokenizer().setInputCols(['sentence']).setOutputCol(\"tokens\")\n",
        "\n",
        "normalizer = Normalizer().setInputCols([\"tokens\"]) \\\n",
        "                         .setOutputCol(\"normalized_tokens\") \\\n",
        "                         .setLowercase(True) \\\n",
        "                         .setCleanupPatterns([\"[^\\w\\d\\s]\"])\n",
        "\n",
        "remove_stopwords = StopWordsCleaner().setInputCols([\"normalized_tokens\"]) \\\n",
        "                                    .setOutputCol(\"clean_tokens\") \\\n",
        "                                    .setCaseSensitive(False)\n",
        "\n",
        "# Get lemmatizer dictionary\n",
        "!wget -q https://raw.githubusercontent.com/mahavivo/vocabulary/master/lemmas/AntBNC_lemmas_ver_001.txt\n",
        "lemmatizer = Lemmatizer().setInputCols([\"clean_tokens\"]) \\\n",
        "                         .setOutputCol(\"lemmatized_token\") \\\n",
        "                         .setDictionary(\"./AntBNC_lemmas_ver_001.txt\", value_delimiter =\"\\t\", key_delimiter = \"->\")\n",
        "\n",
        "finisher = Finisher().setInputCols([\"lemmatized_token\"]) \\\n",
        "                     .setOutputCols(\"lemma_bow\") \\\n",
        "                     .setIncludeMetadata(False)\n",
        "\n",
        "pipeline_stages =[docAssembler, sentence_detector, \n",
        "                  tokenizer, normalizer, remove_stopwords, \n",
        "                  lemmatizer, finisher]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xsdx2wUsxKVO",
        "outputId": "d2974417-5c0e-438c-f6b3-70ba49d5db37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# fitting the preprocessing pipeline to the train data\n",
        "nlpPipeline = Pipeline(stages=pipeline_stages)\n",
        "preprocessingModel = nlpPipeline.fit(train)\n",
        "\n",
        "# using the preprocessing pipeline to transform the train and test split\n",
        "preprocessed_train = preprocessingModel.transform(train)\n",
        "preprocessed_test = preprocessingModel.transform(test)\n",
        "\n",
        "preprocessed_train.printSchema()\n",
        "preprocessed_test.printSchema()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- HEADLINE: string (nullable = true)\n",
            " |-- IS_SARCASTIC: string (nullable = true)\n",
            " |-- lemma_bow: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "root\n",
            " |-- HEADLINE: string (nullable = true)\n",
            " |-- IS_SARCASTIC: string (nullable = true)\n",
            " |-- lemma_bow: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CrCHTY_yXQE",
        "outputId": "654cd326-d6ff-4ada-acc5-c1a8aac8d884",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Identifying the longest sentence (by number of words after preprocessing) and using the size to set the\n",
        "#word2vec vector size.\n",
        "vector_dimension = preprocessed_train.withColumn(\"lemma_bow_size\",size(\"lemma_bow\")).agg({\"lemma_bow_size\":\"max\"}).collect()[0][0]\n",
        "\n",
        "print(vector_dimension)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP8jsqevyp1E"
      },
      "source": [
        "**Building the Spark MLLib Binary Text Classification pipe line**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeCrjn7K0yn7"
      },
      "source": [
        "indexer = StringIndexer(inputCol=\"IS_SARCASTIC\", outputCol=\"label\")\n",
        "\n",
        "word2Vec = Word2Vec(vectorSize= vector_dimension, minCount=0, inputCol=\"lemma_bow\", outputCol=\"features\")\n",
        "\n",
        "classifier = LogisticRegression()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMaVtsiC1rSr"
      },
      "source": [
        "**Training and evaluating a basic logistic regression model for binary classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG9uJzAi00Lh"
      },
      "source": [
        "ml_stages =[indexer,word2Vec,classifier]\n",
        "\n",
        "ml_pipeline = Pipeline(stages=ml_stages)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAkUY6yl1mjq"
      },
      "source": [
        "model = ml_pipeline.fit(preprocessed_train)\n",
        "predictionAndLabels = model.transform(preprocessed_test)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcc2KqCz19Wu"
      },
      "source": [
        "auc_roc_evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction',metricName='areaUnderROC')\n",
        "pr_roc_evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction',metricName='areaUnderPR')\n",
        "precision_evaluator = MulticlassClassificationEvaluator(metricName=\"weightedPrecision\")\n",
        "recall_evaluator = MulticlassClassificationEvaluator(metricName=\"weightedRecall\")\n",
        "f1_evaluator = MulticlassClassificationEvaluator(metricName=\"f1\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0IRzKrX2O3D",
        "outputId": "df37f927-e907-47d5-e786-dab64922efc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"AUC ROC: {0:.2f}\".format(auc_roc_evaluator.evaluate(predictionAndLabels)))\n",
        "print(\"AUC PR: {0:.2f}\".format(pr_roc_evaluator.evaluate(predictionAndLabels)))\n",
        "print(\"Precision: {0:.2f}\".format(precision_evaluator.evaluate(predictionAndLabels)))\n",
        "print(\"Recall ROC: {0:.2f}\".format(recall_evaluator.evaluate(predictionAndLabels)))\n",
        "print(\"F1: {0:.2f}\".format(f1_evaluator.evaluate(predictionAndLabels)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC ROC: 0.60\n",
            "AUC PR: 0.54\n",
            "Precision: 0.62\n",
            "Recall ROC: 0.62\n",
            "F1: 0.61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja956h-Y2T0q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}